\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsthm}
\usepackage{graphicx}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}

\begin{document}

\title{Linear Algebra Hw2}
\author{Toby Harvey}
\maketitle


\noindent \textbf{(3c)} Let $W$ be the space in question.

\vspace{3mm}
\noindent Existence of additive identity:


\noindent The additive identity of $\mathbb{R}^{(-4,4)}$ is $f(x) = 0$ where $x \in (-4,4)$. This is in the space W, because $f^{\prime}(-1) = 3f(2) = 0$.

\vspace{3mm}

\noindent Addition is closed:

\noindent Let $f,g \in W$. Then we have:

$$ (f + g)^{\prime}(-1) = f^{\prime}(-1) + f^{\prime}(-1) = 3f(2) + 3g(2) = 3(f+g)(2)$$

\vspace{3mm}

\noindent Scalar Multiplication is closed:

\noindent Let $\lambda \in \mathbb{R}$, and $f \in W$. Then we have:

$$(\lambda f)^{\prime}(-1) = \lambda f^{\prime}(-1) = \lambda 3f(2) = 3 (\lambda f)(2)$$


\noindent Therefore $W$ is a subspace


\vspace{9mm}


\noindent\textbf{(4c)} Let $W$ be the space in question.

\vspace{3mm}

\noindent The additive identity for $\mathbb{R}^{[0,1]}$ is $f(x) = 0$ where $x \in [0,1]$.
If $f(x) \in W$ we must have:

$$\int_0^1 (f) dx = 0 = b$$

\noindent The only way this is possible is if $b = 0$.

\vspace{3mm}

\noindent Let $f, g \in W$. Then we know:

$$\int_0^1 (f + g) dx = \int_0^1 f dx + \int_0^1 g dx = b + b = 2b$$

\noindent If we want closer under addition we need $\int_0^1 (f + g) dx = b$, and the only way this is possible is if $b=0$.

\newpage

\noindent Let $\lambda \in \mathbb{R}$. For closer under scalar multiplication we have:

$$\int_0^1 (\lambda f) dx = \lambda \int_0^1 fdx = \lambda b$$

\noindent If we want closer under scalar multiplication we need $\int_0^1 (\lambda f) dx = b$, and that is only possible with $b=0$.

\noindent So $W$ is a subspace if $b = 0$.

\noindent Now we must show that the functions in $R^{[0,1]}$ such that $\int_0^1 f dx = 0$, call this space $W$, form a subspace.


\noindent The addative identity of $R^{[0,1]}$, $f(x) = 0$ is infact in $W$ because:

$$\int_0^1 f  dx = 0$$

\noindent Closed under addation: Let $f,g \in W$ then to show closer under addation we have:

$$\int_0^1 (f + g) dx = \int_0^1 f dx + \int_0^1 g dx = 0 + 0 = 0$$

\noindent Closed under scalar multiplication: Let $\lambda \in \mathbb{R}$ and $f \in W$. scalar multiplication is closed because:

$$\int_0^1 (\lambda f) dx = \lambda \int_0^1 fdx = \lambda 0 = 0$$

\noindent Therefore $W$ is a subspace if and only if $b = 0$.


\vspace{9mm}

\noindent\textbf{(5c)}Assume the field on $\mathbb{C}^2$ is $\mathbb{R}$.

\noindent The additive identity for $\mathbb{C}^2$ is $(0 + 0i, 0 + 0i) = (0, 0) \in \mathbb{R}^2$

\noindent Let $(a,b), (c,d) \in \mathbb{R}^2$. Closure under vector addition:

$$(a,b) + (c,d) = ((a+c), (b+d)) \in \mathbb{R}^2$$


\noindent Let $\lambda \in R$. Closure under scalar multiplication:

$$ \lambda (a,b) = (\lambda a, \lambda b) \in \mathbb{R}^2$$


\vspace{3mm}

\noindent\textbf{(7c)} The set:

$$U = \{(x,y) : x \in \mathbb{Z} \text{ and } y \in \mathbb{Z} \}$$

\noindent Addition is closed because if $(x_1, y_1), (x_2, y_2) \in U$, we have $x_1 + x_2 \in \mathbb{Z}$ and $y_1 + y_2 \in \mathbb{Z}$, so $(x_1, y_1) + (x_2, y_2) \in U$. $U$ is closed  under addative inverses, because if $(x,y) \in U$ we have $(x,y) + (-x, -y) = 0$, and $-x_1 \in Z$ and $-y_1 \in \mathbb{Z}$ Therefore $(-x,-y) \in U$. For scalar multiplication assuming the field over $\mathbb{R}^2$ is $\mathbb{R}$. Then letting $a \in \mathbb{R}$, and $(x,y) \in U$,  $a(x,y)$ is not necessarly in $U$ if $a \notin \mathbb{Z}$.


\vspace{3mm}

\noindent\textbf{(15c)}
$$U + U = \{v \in V : v = u_1 + u_2 \text{ where } u_1,u_2 \in U \}$$
Since $U$ is a subspace of $V$, $U$ is closed under addition, so it follows that $v = u_1 + u_2 \in U$, so $U+U \subseteq U$. To show the other inclusion. Since $u_1, u_2 \in U$ we have that either could be equal to the addative identity, $0$, so that $v = u_1 + 0 = u_1$, or $v = u_2 + 0 = u_2$. This shows thats $U \subseteq U + U$. So by double inclusion $U = U+U$.


\vspace{3mm}

\noindent\textbf{(19c)}


\vspace{3mm}



\noindent\textbf{(1a)} If we can find linear combinations of $v_1 - v_2, v_2 - v_3, v_3 - v_4, v_4$, that equal each of $v_1, v_2, v_3, v_4$ then we know that since a linear combination of linear combinations is just another linear combination, and that $v_1, v_2, v_3, v_4$ spans $V$ then $v_1 - v_2, v_2 - v_3, v_3 - v_4, v_4$ must also span $V$. Symbolically we have:

\begin{gather*}
  v_1 = 1(v_1 - v_2) + 1(v_2 - v_3) + 1(v_3 - v_4) + 1(v_4) \\
  v_2 = 0(v_1 - v_2) + 1(v_2 - v_3) + 1(v_3 - v_4) + 1(v_4) \\
  v_3 = 0(v_1 - v_2) + 0(v_2 - v_3) + 1(v_3 - v_4) + 1(v_4) \\
  v_4 = 0(v_1 - v_2) + 0(v_2 - v_3) + 0(v_3 - v_4) + 1(v_4) \\
\end{gather*}

\noindent Now since we have new expressions for $v_1, v_2, v_3, v_4$, we can rewrite a linear combination of these vectors as an arbitrary vector $v \in V$ as:

\begin{gather*}
  v = c_1(1(v_1 - v_2) + 1(v_2 - v_3) + 1(v_3 - v_4) + 1(v_4))\\
  + c_2(1(v_2 - v_3) + 1(v_3 - v_4) + 1(v_4)) \\
  + c_3(1(v_3 - v_4) + 1(v_4)) \\
  + c_4(1(v_4)) \\
\end{gather*}

\noindent Factoring out the 1 here we see that this new set of vectors also spans $V$.

\vspace{3mm}

\noindent\textbf{(3A)} If $(3,1,4), (2,-3,5), (5,9,t)$ are not linearly indepedent we can try to look for $a_1, a_2, a_3$ not 0 such that:

$$a_1(3,1,4) + a_2(2,-3,5) = (5,9,t)$$

or:

\begin{gather*}
  3a_1 + 2a_2 = 5\\
  a_1 - 3a_2  = 9 \\
  4a_1 + 5a_2 = t\\
\end{gather*}

Manipulating the first equation to put $a_1$ alone on one side and plugging it into the second equation we have:

$$\frac{5}{3} - \frac{2}{3}a_2 - 3a_2 = 9$$

or $a_2 = -2$, plugging this back into the first equation we get $a_1 = 3$. Plugging these both into the third equation we get $t = 2$.




\vspace{3mm}

\noindent\textbf{(5a)} a. If $(1 + i, 1 - i)$ is linearly indepedent assuming $a_1,a_2 \in \mathbb{R}$, then the only solution to $a_1(1+i) + a_2(1-i) = 0$ or
\begin{gather*}
  a_1 + a_2 = 0 \\
  a_1 - a_2 = 0 \\
\end{gather*}

\noindent is if $a_1 = a_2 = 0$. Solving this system we see that $a_1 = -a_2$, and $a_1 = a_2$ The only number this is true of  is 0, therefore $(1 + i, 1 - i)$ is linearly independent.

\vspace{3mm}

\noindent b. If $(1+i, 1-i)$ are linearly dependent assuming $a_1,a_2 \in \mathbb{C}$ There must be $a_1, a_2$ not both 0 such that $a_1(1+i) + a_2(1-i) = 0$. Letting $a_1 = i$ and $a_2 = 1$ we see that $i(1+i) + 1(1 - i) = (-1 + i) + (1 - i) = 0$. So $(1+i, 1-i)$ is linearly dependent.

\vspace{3mm}

\noindent\textbf{(8a)} We must show that $a_1 = a_2 = ... = a_m = 0$ is the only way that $a_1 (\lambda v_1) + a_2 (\lambda v_2) + ... + a_m (\lambda v_m) = 0$ where $\lambda \neq 0$ and $\lambda \in \mathbb{F}$. Factoring the $\lambda$ we have $a_1 \lambda (v_1) + a_2 \lambda (v_2) + ... + a_m \lambda (v_m) = 0$ Since $v_1, v_2, ... v_m$ are linearly indepedent we have that $a_1 \lambda = a_2 \lambda = ... =a_m \lambda = 0$, but by assumption $\lambda \neq 0$ which means that $a_1 = a_2 = ... = a_m = 0$, and therefore $\lambda v_1, \lambda v_2, ... \lambda v_m$ are linearly independent.

\vspace{3mm}

\noindent\textbf{(11a)} Assume that $v_1,...v_m, w$ are linearly independent, then we must only have that $a_1 = ... =a_m = a_w = 0$ in the equation $a_1v_1 + ... + a_mv_m + a_ww = 0$. Now assume that we can write that $w \in \text{span}(v_1,...,v_m)$ or that $w = a_1v_1 + ... +a_mv_m$. If we subtract $w$ from both sides we get $0 = a_1v_1 + ... a_mv_m - w$ factoring out a -1 we see that $0 = a_1v_1 + ... a_mV_m + -1(w)$ which contradicts that $a_1 = ... =a_m = a_w = 0$, because here $a_w = -1$.

\noindent Now assume that $w \notin \text{span}(v_1,...,v_m)$, so that there is no list of $a_1...a_m$ such that we can write $w = a_1v_1 + ... +a_mv_m$, and that $v_1,...,v_m$ are linearly indepedent. Also assume, by way of contradiction that $v_1,...,v_m,w$ are linearly depedent so that there are coefficents $a_1,...,a_m,a_w$ not all equal to zero such that $a_1v_1 + ... + a_mv_m + a_ww = 0$.

\vspace{3mm}

\noindent If $a_w \neq 0$ then manipulating the last equation we have $w = -\frac{a_1}{a_w}v_1 - ... - \frac{a_m}{a_w}v_m$, which contradicts that  $w \notin \text{span}(v_1,...,v_m)$.


\noindent If $a_w = 0$, then by the hypothesis that $v_1,..,v_m$, and $a_w = 0$ we have that $v_1,...v_m,w$ are linearly independent which is what we wanted to prove.

\newpage

\noindent\textbf{(14a)} Let $V$ be an infinite-dimensional vector space. And let $(v_1,...,v_n)$ be a linearly indepedent list of vectors in $V$. Since $V$ is infinite-dimensional by definition $(v_1,...,v_n)$ does not span $V$, we can then add a vector $v_{n+1}$ to $(v_1,...,v_n)$ so we have $(v_1,...,v_n, v_{n+1})$, and again we will not space $V$ with this list, continuing onwards we see that this is true of every integer $m$ where $m$ is the number of vectors in our list.

\vspace{3mm}

\noindent Now assume that there is a list of linearly indepedent vectors in $V$ with length $m$ for every positive integer $m$. Assume that for a particular $m$ $v_1,...,v_m$ spans $V$. By 2.23 in our book though we have that the length of every linearly indepedent list of vectors is less than or equal to the length of every spanning list of vectors. By assumption we have a list of linearly indepedent vectors of length $m+1$, which would imply that the length of $v_1,...,v_m$ is less than the length of $v_1,...,v_{m+1}$ which is a contradiction, and therefore there is no particular set of vectors $v_1,...,v_m$ that spans $V$, and $V$ must be finite dimensional.



\end{document}
